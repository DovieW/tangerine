//! Local Whisper STT provider using whisper.cpp bindings.
//!
//! This module provides offline speech-to-text using the whisper-rs library
//! (Rust bindings for whisper.cpp). It's an optional feature that requires
//! the `local-whisper` feature flag to be enabled.
//!
//! ## Requirements
//! - whisper.cpp model files (downloaded separately)
//! - Feature flag: `--features local-whisper`
//!
//! ## Model Sizes
//! - tiny: ~75MB, fastest, lower accuracy
//! - base: ~142MB, good balance
//! - small: ~466MB, better accuracy
//! - medium: ~1.5GB, high accuracy
//! - large: ~2.9GB, highest accuracy

use super::{AudioFormat, SttError, SttProvider};
use async_trait::async_trait;
use std::path::PathBuf;
use std::sync::Arc;
use whisper_rs::{FullParams, SamplingStrategy, WhisperContext, WhisperContextParameters};

/// Available Whisper model sizes
#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub enum WhisperModel {
    Tiny,
    TinyEn,
    Base,
    BaseEn,
    Small,
    SmallEn,
    Medium,
    MediumEn,
    LargeV1,
    LargeV2,
    LargeV3,
    LargeV3Turbo,
}

impl WhisperModel {
    /// Get the model filename
    pub fn filename(&self) -> &'static str {
        match self {
            Self::Tiny => "ggml-tiny.bin",
            Self::TinyEn => "ggml-tiny.en.bin",
            Self::Base => "ggml-base.bin",
            Self::BaseEn => "ggml-base.en.bin",
            Self::Small => "ggml-small.bin",
            Self::SmallEn => "ggml-small.en.bin",
            Self::Medium => "ggml-medium.bin",
            Self::MediumEn => "ggml-medium.en.bin",
            Self::LargeV1 => "ggml-large-v1.bin",
            Self::LargeV2 => "ggml-large-v2.bin",
            Self::LargeV3 => "ggml-large-v3.bin",
            Self::LargeV3Turbo => "ggml-large-v3-turbo.bin",
        }
    }

    /// Get the Hugging Face download URL
    pub fn download_url(&self) -> String {
        format!(
            "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/{}",
            self.filename()
        )
    }

    /// Get approximate model size in bytes
    pub fn size_bytes(&self) -> u64 {
        match self {
            Self::Tiny | Self::TinyEn => 75_000_000,
            Self::Base | Self::BaseEn => 142_000_000,
            Self::Small | Self::SmallEn => 466_000_000,
            Self::Medium | Self::MediumEn => 1_500_000_000,
            Self::LargeV1 | Self::LargeV2 | Self::LargeV3 => 2_900_000_000,
            Self::LargeV3Turbo => 1_600_000_000,
        }
    }

    /// Get human-readable model name
    pub fn display_name(&self) -> &'static str {
        match self {
            Self::Tiny => "Tiny (75MB)",
            Self::TinyEn => "Tiny English (75MB)",
            Self::Base => "Base (142MB)",
            Self::BaseEn => "Base English (142MB)",
            Self::Small => "Small (466MB)",
            Self::SmallEn => "Small English (466MB)",
            Self::Medium => "Medium (1.5GB)",
            Self::MediumEn => "Medium English (1.5GB)",
            Self::LargeV1 => "Large v1 (2.9GB)",
            Self::LargeV2 => "Large v2 (2.9GB)",
            Self::LargeV3 => "Large v3 (2.9GB)",
            Self::LargeV3Turbo => "Large v3 Turbo (1.6GB)",
        }
    }

    /// Check if this is an English-only model
    pub fn is_english_only(&self) -> bool {
        matches!(
            self,
            Self::TinyEn | Self::BaseEn | Self::SmallEn | Self::MediumEn
        )
    }

    /// List all available models
    pub fn all() -> Vec<Self> {
        vec![
            Self::Tiny,
            Self::TinyEn,
            Self::Base,
            Self::BaseEn,
            Self::Small,
            Self::SmallEn,
            Self::Medium,
            Self::MediumEn,
            Self::LargeV1,
            Self::LargeV2,
            Self::LargeV3,
            Self::LargeV3Turbo,
        ]
    }
}

impl Default for WhisperModel {
    fn default() -> Self {
        Self::Base // Good balance of speed and accuracy
    }
}

/// Configuration for the local Whisper provider
#[derive(Debug, Clone)]
pub struct LocalWhisperConfig {
    /// Path to the model file
    pub model_path: PathBuf,
    /// Language to use (None for auto-detect)
    pub language: Option<String>,
    /// Whether to translate to English
    pub translate: bool,
    /// Number of threads to use (0 = auto)
    pub n_threads: u32,
}

impl Default for LocalWhisperConfig {
    fn default() -> Self {
        Self {
            model_path: PathBuf::new(),
            language: Some("en".to_string()),
            translate: false,
            n_threads: 0, // Auto-detect
        }
    }
}

/// Local Whisper STT provider
pub struct LocalWhisperProvider {
    ctx: Arc<WhisperContext>,
    config: LocalWhisperConfig,
}

impl LocalWhisperProvider {
    /// Create a new provider with the given model path
    pub fn new(model_path: PathBuf) -> Result<Self, SttError> {
        Self::with_config(LocalWhisperConfig {
            model_path,
            ..Default::default()
        })
    }

    /// Create a new provider with custom configuration
    pub fn with_config(config: LocalWhisperConfig) -> Result<Self, SttError> {
        if !config.model_path.exists() {
            return Err(SttError::Audio(format!(
                "Model file not found: {}",
                config.model_path.display()
            )));
        }

        let ctx_params = WhisperContextParameters::default();

        let ctx = WhisperContext::new_with_params(
            config.model_path.to_str().ok_or_else(|| {
                SttError::Audio("Invalid model path encoding".to_string())
            })?,
            ctx_params,
        )
        .map_err(|e| SttError::Audio(format!("Failed to load Whisper model: {}", e)))?;

        Ok(Self {
            ctx: Arc::new(ctx),
            config,
        })
    }

    /// Check if a model file exists at the given path
    pub fn model_exists(model_path: &PathBuf) -> bool {
        model_path.exists() && model_path.is_file()
    }

    /// Get the default models directory
    pub fn default_models_dir() -> Option<PathBuf> {
        dirs::data_local_dir().map(|d| {
            d.join("tangerine-voice").join("models")
        })
    }
}

#[async_trait]
impl SttProvider for LocalWhisperProvider {
    async fn transcribe(&self, audio: &[u8], _format: &AudioFormat) -> Result<String, SttError> {
        // Decode WAV to f32 samples
        let samples = decode_wav_to_f32_mono_16khz(audio)?;

        if samples.is_empty() {
            return Ok(String::new());
        }

        // Clone what we need for the blocking task
        let ctx = self.ctx.clone();
        let language = self.config.language.clone();
        let translate = self.config.translate;
        let n_threads = self.config.n_threads;

        // whisper-rs is synchronous, so we use spawn_blocking
        let result = tokio::task::spawn_blocking(move || {
            let mut state = ctx
                .create_state()
                .map_err(|e| SttError::Audio(format!("Failed to create Whisper state: {}", e)))?;

            let mut params = FullParams::new(SamplingStrategy::Greedy { best_of: 1 });

            // Set language
            if let Some(lang) = &language {
                params.set_language(Some(lang));
            }

            // Set translate mode
            params.set_translate(translate);

            // Set thread count
            if n_threads > 0 {
                params.set_n_threads(n_threads as i32);
            }

            // Disable printing to reduce noise
            params.set_print_special(false);
            params.set_print_progress(false);
            params.set_print_realtime(false);
            params.set_print_timestamps(false);

            // Run inference
            state
                .full(params, &samples)
                .map_err(|e| SttError::Audio(format!("Whisper inference failed: {}", e)))?;

            // Collect results
            let num_segments = state.full_n_segments().map_err(|e| {
                SttError::Audio(format!("Failed to get segment count: {}", e))
            })?;

            let mut text = String::new();
            for i in 0..num_segments {
                if let Ok(segment_text) = state.full_get_segment_text(i) {
                    text.push_str(&segment_text);
                }
            }

            Ok::<String, SttError>(text.trim().to_string())
        })
        .await
        .map_err(|e| SttError::Audio(format!("Task join error: {}", e)))??;

        Ok(result)
    }

    fn name(&self) -> &'static str {
        "local-whisper"
    }
}

/// Decode WAV audio to f32 samples, converting to mono 16kHz if needed
fn decode_wav_to_f32_mono_16khz(wav_bytes: &[u8]) -> Result<Vec<f32>, SttError> {
    use std::io::Cursor;

    let cursor = Cursor::new(wav_bytes);
    let mut reader = hound::WavReader::new(cursor)
        .map_err(|e| SttError::Audio(format!("Failed to read WAV: {}", e)))?;

    let spec = reader.spec();
    let sample_rate = spec.sample_rate;
    let channels = spec.channels as usize;

    // Read samples based on format
    let samples: Vec<f32> = match spec.sample_format {
        hound::SampleFormat::Float => reader
            .samples::<f32>()
            .map(|s| s.unwrap_or(0.0))
            .collect(),
        hound::SampleFormat::Int => {
            let bits = spec.bits_per_sample;
            let max_val = (1 << (bits - 1)) as f32;
            reader
                .samples::<i32>()
                .map(|s| s.unwrap_or(0) as f32 / max_val)
                .collect()
        }
    };

    // Convert to mono if stereo
    let mono_samples: Vec<f32> = if channels > 1 {
        samples
            .chunks(channels)
            .map(|chunk| chunk.iter().sum::<f32>() / channels as f32)
            .collect()
    } else {
        samples
    };

    // Resample to 16kHz if needed
    if sample_rate != 16000 {
        Ok(crate::vad::resample_to_16khz(&mono_samples, sample_rate))
    } else {
        Ok(mono_samples)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_model_filenames() {
        assert_eq!(WhisperModel::Tiny.filename(), "ggml-tiny.bin");
        assert_eq!(WhisperModel::BaseEn.filename(), "ggml-base.en.bin");
        assert_eq!(WhisperModel::LargeV3.filename(), "ggml-large-v3.bin");
    }

    #[test]
    fn test_model_urls() {
        let url = WhisperModel::Base.download_url();
        assert!(url.contains("huggingface.co"));
        assert!(url.contains("ggml-base.bin"));
    }

    #[test]
    fn test_english_only_models() {
        assert!(WhisperModel::TinyEn.is_english_only());
        assert!(WhisperModel::BaseEn.is_english_only());
        assert!(!WhisperModel::Tiny.is_english_only());
        assert!(!WhisperModel::LargeV3.is_english_only());
    }

    #[test]
    fn test_all_models() {
        let models = WhisperModel::all();
        assert!(models.len() >= 10);
    }
}
